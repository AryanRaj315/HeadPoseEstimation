{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HeadPoseEstimation",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3636921813bd4d3e9e9be41688187a65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_964252aad4fc4e5eb4492760737c9cfc",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_fe6be417072344a59d96d1e93654f5ca",
              "IPY_MODEL_55c3a1b0d895423ebc45de0b8cb0303a"
            ]
          }
        },
        "964252aad4fc4e5eb4492760737c9cfc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fe6be417072344a59d96d1e93654f5ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d6e0735d5a8c4e3690e5aabb9134f748",
            "_dom_classes": [],
            "description": "",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 15678,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 15678,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_dd28c83296b64c5baf8f31e3e5fec29d"
          }
        },
        "55c3a1b0d895423ebc45de0b8cb0303a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_81d87020c654468c9da1c6776721ba91",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 15678/15678 [03:20&lt;00:00, 78.25it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1acaf8c7dbac409a8d74621b619a9ca5"
          }
        },
        "d6e0735d5a8c4e3690e5aabb9134f748": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "dd28c83296b64c5baf8f31e3e5fec29d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "81d87020c654468c9da1c6776721ba91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1acaf8c7dbac409a8d74621b619a9ca5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8ffb9981478346e09257e8f580798c15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_801be22d2c534de0aaddd0badb785f49",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a270151754e644d09fae43c76f38aaf4",
              "IPY_MODEL_e558b817f5644b2cb64f3c99808a4b07"
            ]
          }
        },
        "801be22d2c534de0aaddd0badb785f49": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a270151754e644d09fae43c76f38aaf4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d2edef737ce445a6a6663a831687b05a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 15678,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 15678,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bce243d9243d4102bb1fba84d9038672"
          }
        },
        "e558b817f5644b2cb64f3c99808a4b07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3ba6f1b4ea394814a03c29aeb269dc30",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 15678/15678 [00:56&lt;00:00, 276.50it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3bb8c89705ce4f9099b3c981a891e5f7"
          }
        },
        "d2edef737ce445a6a6663a831687b05a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bce243d9243d4102bb1fba84d9038672": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3ba6f1b4ea394814a03c29aeb269dc30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3bb8c89705ce4f9099b3c981a891e5f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "IryROr9SmZTe",
        "colab_type": "code",
        "outputId": "034605c2-cad7-4774-d6fd-a7e0f2cf970a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfjLcphiY20p",
        "colab_type": "text"
      },
      "source": [
        "# Getting the dataset and installing necessary libraries\n",
        "Dataset link : https://data.vision.ee.ethz.ch/cvl/gfanelli/head_pose/head_forest.html#db"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kw8pJITNfrIl",
        "colab_type": "code",
        "outputId": "d4448af3-61a8-4970-80ca-db538779ee1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "!wget http://data.vision.ee.ethz.ch/cvl/gfanelli/kinect_head_pose_db.tgz"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-12-04 06:40:59--  http://data.vision.ee.ethz.ch/cvl/gfanelli/kinect_head_pose_db.tgz\n",
            "Resolving data.vision.ee.ethz.ch (data.vision.ee.ethz.ch)... 129.132.52.162\n",
            "Connecting to data.vision.ee.ethz.ch (data.vision.ee.ethz.ch)|129.132.52.162|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://data.vision.ee.ethz.ch/cvl/gfanelli/kinect_head_pose_db.tgz [following]\n",
            "--2019-12-04 06:40:59--  https://data.vision.ee.ethz.ch/cvl/gfanelli/kinect_head_pose_db.tgz\n",
            "Connecting to data.vision.ee.ethz.ch (data.vision.ee.ethz.ch)|129.132.52.162|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6014398431 (5.6G) [application/x-gzip]\n",
            "Saving to: ‘kinect_head_pose_db.tgz’\n",
            "\n",
            "kinect_head_pose_db 100%[===================>]   5.60G  15.7MB/s    in 6m 0s   \n",
            "\n",
            "2019-12-04 06:46:59 (15.9 MB/s) - ‘kinect_head_pose_db.tgz’ saved [6014398431/6014398431]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hC4afLsOaE0-",
        "colab_type": "text"
      },
      "source": [
        "Extracting the .tgz file "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnBw54DqfvMV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!tar -xf '/content/kinect_head_pose_db.tgz'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5JmXRWwfDjN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import cv2\n",
        "from math import atan2, asin \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import dlib\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "from torch.utils.data import DataLoader, Dataset, sampler\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from albumentations.pytorch import ToTensor\n",
        "from albumentations import (HorizontalFlip, ShiftScaleRotate, Normalize, Resize, Compose, GaussNoise)\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import torch.optim as optim\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torchvision.models as models\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNjjv_XuZIPN",
        "colab_type": "code",
        "outputId": "0e04b4ae-fe3c-42a1-b801-b0b877f7d744",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        }
      },
      "source": [
        "!wget http://dlib.net/files/mmod_human_face_detector.dat.bz2\n",
        "!7z x \"mmod_human_face_detector.dat.bz2\"\n",
        "detector = dlib.cnn_face_detection_model_v1(\"mmod_human_face_detector.dat\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-12-04 07:17:32--  http://dlib.net/files/mmod_human_face_detector.dat.bz2\n",
            "Resolving dlib.net (dlib.net)... 107.180.26.78\n",
            "Connecting to dlib.net (dlib.net)|107.180.26.78|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 694709 (678K)\n",
            "Saving to: ‘mmod_human_face_detector.dat.bz2.1’\n",
            "\n",
            "mmod_human_face_det 100%[===================>] 678.43K   564KB/s    in 1.2s    \n",
            "\n",
            "2019-12-04 07:17:34 (564 KB/s) - ‘mmod_human_face_detector.dat.bz2.1’ saved [694709/694709]\n",
            "\n",
            "\n",
            "7-Zip [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21\n",
            "p7zip Version 16.02 (locale=en_US.UTF-8,Utf16=on,HugeFiles=on,64 bits,2 CPUs Intel(R) Xeon(R) CPU @ 2.30GHz (306F0),ASM,AES-NI)\n",
            "\n",
            "Scanning the drive for archives:\n",
            "  0M Scan\b\b\b\b\b\b\b\b\b         \b\b\b\b\b\b\b\b\b1 file, 694709 bytes (679 KiB)\n",
            "\n",
            "Extracting archive: mmod_human_face_detector.dat.bz2\n",
            "--\n",
            "Path = mmod_human_face_detector.dat.bz2\n",
            "Type = bzip2\n",
            "\n",
            "\n",
            "Would you like to replace the existing file:\n",
            "  Path:     ./mmod_human_face_detector.dat\n",
            "  Size:     729940 bytes (713 KiB)\n",
            "  Modified: 2016-10-08 02:01:00\n",
            "with the file from archive:\n",
            "  Path:     mmod_human_face_detector.dat\n",
            "? (Y)es / (N)o / (A)lways / (S)kip all / A(u)to rename all / (Q)uit? A\n",
            "\n",
            "  0% - mmod_human_face_detector.dat\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                   \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bEverything is Ok\n",
            "\n",
            "Size:       729940\n",
            "Compressed: 694709\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzmazpEIZIqP",
        "colab_type": "text"
      },
      "source": [
        "# Useful functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IowtBOkKcJGs",
        "colab_type": "text"
      },
      "source": [
        "Function to get the path"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pk0MxqldjQYH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def PathList(path):\n",
        "  Img_Path = []\n",
        "  for root, dirs, files in os.walk(path, topdown=False):\n",
        "      for filename in files:\n",
        "        if(filename[-3:] == 'png'):\n",
        "          Img_Path.append(root+'/'+filename)\n",
        "  return Img_Path        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMEeRsYkZe34",
        "colab_type": "text"
      },
      "source": [
        "Here ImagesPath is a list that contains all the .png files from the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asB6nok3Zdx6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ImagesPath = PathList(\"hpdb/\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Btbc9Ol-Zni2",
        "colab_type": "text"
      },
      "source": [
        "This function is used to convert the transformation matrix from \n",
        "all the .txt files to Yaw, Pitch and Roll "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHZrxlFBZWYK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getAngles(path):\n",
        "  rot_mat = []\n",
        "  with open(path, \"r\") as f:\n",
        "    l = f.read()\n",
        "    l = l.split(\"\\n\")\n",
        "    for i in range(0, 3): \n",
        "      rot_mat.append(l[i].split(\" \")[:3])\n",
        "    rot_mat = np.asarray(rot_mat)\n",
        "    rot_mat = rot_mat.astype(float) \n",
        "    roll = atan2(rot_mat[2][1], rot_mat[2][2])*(180/math.pi);\n",
        "    pitch = asin(rot_mat[2][0])*(180/math.pi);\n",
        "    yaw = -atan2(rot_mat[1][0], rot_mat[0][0])*(180/math.pi);\n",
        "  return yaw, pitch, roll "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Ta4UV4Yaiux",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "29fac850-466b-4849-cb59-93275d8952d2"
      },
      "source": [
        "!mkdir cropped_Images"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘cropped_Images’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozn0FvGV6Yqo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(1,25):\n",
        "  path = \"cropped_Images/\"+\"{:02d}\".format(i)\n",
        "  if not os.path.exists(path):\n",
        "    os.mkdir(path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VCV9ZE-LIkwk",
        "colab": {}
      },
      "source": [
        "a = np.load(\"/content/drive/My Drive/CropSizeList.npy\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VLRSx4ephBz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def SaveImages(CropSizeList, ImagesPath):\n",
        "  for i in tqdm(range(len(ImagesPath))):\n",
        "     PersonNumber = int(ImagesPath[i][5:7])\n",
        "     top, bottom, left, right = CropSizeList[PersonNumber - 1] \n",
        "     img = cv2.imread(ImagesPath[i])[top:bottom, left:right]\n",
        "     cv2.imwrite(\"cropped_Images/\" + ImagesPath[i][5:], img)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOhPTnQwq-CF",
        "colab_type": "code",
        "outputId": "6d718caf-2bdc-4741-92ec-427748438d7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "3636921813bd4d3e9e9be41688187a65",
            "964252aad4fc4e5eb4492760737c9cfc",
            "fe6be417072344a59d96d1e93654f5ca",
            "55c3a1b0d895423ebc45de0b8cb0303a",
            "d6e0735d5a8c4e3690e5aabb9134f748",
            "dd28c83296b64c5baf8f31e3e5fec29d",
            "81d87020c654468c9da1c6776721ba91",
            "1acaf8c7dbac409a8d74621b619a9ca5"
          ]
        }
      },
      "source": [
        "SaveImages(a, ImagesPath)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3636921813bd4d3e9e9be41688187a65",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=15678), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-HK9CCcaNJb",
        "colab_type": "text"
      },
      "source": [
        "This function is used to crop out unnecessary background from the .png files which will later be used for the face extraction "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzH2JcdvLz0I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def crop():\n",
        "#   j = 1\n",
        "#   k = 1\n",
        "#   rows = 24\n",
        "#   columns = 2\n",
        "#   fig=plt.figure(figsize=(20, 100))\n",
        "#   for i in tqdm(range(1, rows*columns + 1)):\n",
        "#     if i%2 == 0:\n",
        "#         img = plt.imread(\"hpdb/\" + \"{:02d}\".format(j) + \"/frame_00005_rgb.png\")\n",
        "#         j += 1\n",
        "#     else:\n",
        "#         img = plt.imread(\"hpdb/\" + \"{:02d}\".format(k) + \"/frame_00005_rgb.png\")[65:-55,200:540]\n",
        "#         k += 1\n",
        "#     fig.add_subplot(rows, columns, i)\n",
        "#     plt.imshow(img,cmap = 'gray')\n",
        "#   plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2GAoP8SiDSG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# crop()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ursQx83NKNNN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !rm -r cropped_Images"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POPhqLQ0cV7j",
        "colab_type": "text"
      },
      "source": [
        "Making a seperate directory to store the faces"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5oixSc8cak3",
        "colab_type": "text"
      },
      "source": [
        "This function will get the faces and save them in the directory created above"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewwWC3kGJcKl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def ExtractAndSaveFaces(detector,ImagesPath):\n",
        "#     jpg_crop_path = []\n",
        "#     count = 0\n",
        "#     count_fail = 0\n",
        "#     for i in tqdm(range(len(ImagesPath))):\n",
        "#         image = cv2.imread(ImagesPath[i])[100:-75,200:540]\n",
        "#         dets = detector(image, 0)\n",
        "#         if len(dets) == 1: \n",
        "#             crop_img = image[dets[0].rect.top()-30:dets[0].rect.bottom()+30,dets[0].rect.left()-30:dets[0].rect.right()+30] # cropping the image with 15 extra pixels on all side\n",
        "#             if crop_img.size == 0:\n",
        "#                 count_fail+=1\n",
        "#                 continue\n",
        "#             cv2.imwrite(\"cropped_Images/\" + ImagesPath[i][5:] , crop_img) # writing the image to directory\n",
        "#             jpg_crop_path.append(\"cropped_Images/\" + ImagesPath[i][5:])\n",
        "#             count = count+1\n",
        "#     print(str(count) + \" image written\")\n",
        "#     print(str(count_fail) + \" image failed\")\n",
        "#     return jpg_crop_path"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRkF-fmgpt0K",
        "colab_type": "text"
      },
      "source": [
        "Getting the path of final images that will be fed to the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44clDxZwrWkm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CroppedImagesPath = PathList(\"cropped_Images/\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Grqp7QbGP7jf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CroppedImagesPath = ExtractAndSaveFaces(detector,ImagesPath)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qz6_ujSOcp0u",
        "colab_type": "text"
      },
      "source": [
        "This function will return a dataframe which contains the image id along with its yaw pitch and roll "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haLaP3HiqTld",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getDataframe(CroppedImagesPath):\n",
        "    df_train = pd.DataFrame(columns = ['Person','image','pitch','yaw','roll'])\n",
        "    df_val = pd.DataFrame(columns = ['Person','image','pitch','yaw','roll'])\n",
        "    df_test = pd.DataFrame(columns = ['Person','image','pitch','yaw','roll'])\n",
        "    for i in tqdm(range(len(CroppedImagesPath))):\n",
        "        yaw, pitch, roll = getAngles(\"hpdb/\"+CroppedImagesPath[i][15:30]+\"pose.txt\")\n",
        "        if(int(CroppedImagesPath[i][15:17]) < 17):\n",
        "          df_train = df_train.append({'Person':CroppedImagesPath[i][15:17],'image':CroppedImagesPath[i][18:29],'pitch':pitch,'yaw':yaw,'roll':roll},ignore_index = True)\n",
        "        elif(int(CroppedImagesPath[i][15:17]) >= 17 and int(CroppedImagesPath[i][15:17]) <= 20):\n",
        "          df_val = df_val.append({'Person':CroppedImagesPath[i][15:17],'image':CroppedImagesPath[i][18:29],'pitch':pitch,'yaw':yaw,'roll':roll},ignore_index = True)  \n",
        "        else:\n",
        "          df_test = df_test.append({'Person':CroppedImagesPath[i][15:17],'image':CroppedImagesPath[i][18:29],'pitch':pitch,'yaw':yaw,'roll':roll},ignore_index = True)  \n",
        "    return df_train, df_val, df_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqjPRxQg7V4h",
        "colab_type": "text"
      },
      "source": [
        "# Dataset Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9oMnTqGGiZ4W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BIWIDataset(Dataset):\n",
        "    def __init__(self, df, path, phase):\n",
        "        self.df = df\n",
        "        self.phase = phase\n",
        "        self.transform = ToTensor()\n",
        "        self.path = path\n",
        "        self.train = [path[i] for i in range(len(path)) if int(path[i][15:17]) <= 16]\n",
        "        self.val = [path[i] for i in range(len(path)) if int(path[i][15:17]) > 16 and int(path[i][15:17]) <= 20]\n",
        "        self.fnames = self.df.index.tolist()\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if(self.phase == 'train'):\n",
        "          Image = cv2.imread(self.train[idx])\n",
        "          Angle = np.array(self.df.iloc[idx,2:])\n",
        "        elif(self.phase == 'val'):\n",
        "          Image = cv2.imread(self.val[idx])\n",
        "          Angle = np.array(self.df.iloc[idx,2:])\n",
        "        Angle = Angle.astype('float64')\n",
        "        Image = cv2.resize(Image,(120,120))\n",
        "        Image = Image.transpose(2,0,1)  # Making Channel First \n",
        "        Image = torch.from_numpy(Image).type(torch.FloatTensor)\n",
        "        Angle = torch.from_numpy(Angle).type(torch.FloatTensor)  \n",
        "        return Image, Angle\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.fnames)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bt2YH2U8uB7v",
        "colab_type": "code",
        "outputId": "060979c1-5955-4c6e-a843-9cdbe4c75d64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "8ffb9981478346e09257e8f580798c15",
            "801be22d2c534de0aaddd0badb785f49",
            "a270151754e644d09fae43c76f38aaf4",
            "e558b817f5644b2cb64f3c99808a4b07",
            "d2edef737ce445a6a6663a831687b05a",
            "bce243d9243d4102bb1fba84d9038672",
            "3ba6f1b4ea394814a03c29aeb269dc30",
            "3bb8c89705ce4f9099b3c981a891e5f7"
          ]
        }
      },
      "source": [
        "df_train, df_val, df_test = getDataframe(CroppedImagesPath)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8ffb9981478346e09257e8f580798c15",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=15678), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmlOBO6f7d1i",
        "colab_type": "text"
      },
      "source": [
        "This function provides the training and validation dataloader accordingly"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJo17moB8K8F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def provider(path, phase,df_train, df_val,  batch_size=8, num_workers=0):    \n",
        "    df = df_train if phase == \"train\" else df_val\n",
        "    image_dataset = BIWIDataset(df, path, phase)\n",
        "    dataloader = DataLoader(\n",
        "        image_dataset,\n",
        "        batch_size=batch_size,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True,\n",
        "        shuffle=True,   \n",
        "    )\n",
        "    return dataloader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSicllGN7lnd",
        "colab_type": "text"
      },
      "source": [
        "# Trainer Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yshhTEvEiarj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Trainer(object):\n",
        "    '''This class takes care of training and validpathation of our model'''\n",
        "    def __init__(self, model, path, df_train, df_val):\n",
        "        self.num_workers = 0\n",
        "        self.batch_size = {\"train\": 16, \"val\": 4}\n",
        "        self.accumulation_steps = 32 // self.batch_size['train']\n",
        "        self.lr = 5e-5\n",
        "        self.num_epochs = 20\n",
        "        self.best_loss = float(\"inf\")\n",
        "        self.phases = [\"train\", \"val\"]\n",
        "        self.device = torch.device(\"cuda:0\")\n",
        "        torch.set_default_tensor_type(\"torch.cuda.FloatTensor\")\n",
        "        self.net = model\n",
        "        self.df_train = df_train\n",
        "        self.df_val = df_val\n",
        "        self.criterion = torch.nn.L1Loss()\n",
        "        self.optimizer = optim.Adam(self.net.parameters(), lr=self.lr)\n",
        "        self.scheduler = ReduceLROnPlateau(self.optimizer, mode=\"min\", patience=3, verbose=True)\n",
        "        self.net = self.net.to(self.device)\n",
        "        cudnn.benchmark = True\n",
        "        self.dataloaders = {\n",
        "            phase: provider(\n",
        "                path = path,\n",
        "                phase=phase,\n",
        "                df_train = df_train,\n",
        "                df_val = df_val,\n",
        "                batch_size=self.batch_size[phase],\n",
        "                num_workers=self.num_workers\n",
        "            )\n",
        "            for phase in self.phases\n",
        "        }\n",
        "        self.losses = {phase: [] for phase in self.phases}\n",
        "        \n",
        "    def forward(self, images, targets):\n",
        "        images = images.to(self.device)\n",
        "        target = targets.to(self.device)\n",
        "        outputs = self.net(images)\n",
        "        loss = self.criterion(outputs, target)\n",
        "        return loss, outputs\n",
        "\n",
        "    def iterate(self, epoch, phase):\n",
        "        start = time.strftime(\"%H:%M:%S\")\n",
        "        print(f\"Starting epoch: {epoch} | phase: {phase} | ⏰: {start}\")\n",
        "        batch_size = self.batch_size[phase]\n",
        "        self.net.train(phase == \"train\")\n",
        "        dataloader = self.dataloaders[phase]\n",
        "        running_loss = 0.0\n",
        "        total_batches = len(dataloader)\n",
        "        tk0 = tqdm(dataloader, total=total_batches)\n",
        "        self.optimizer.zero_grad()\n",
        "        for itr, batch in enumerate(tk0): # replace `dataloader` with `tk0` for tqdm\n",
        "            images, targets = batch\n",
        "            loss, outputs = self.forward(images, targets)\n",
        "            loss = loss / self.accumulation_steps\n",
        "            if phase == \"train\":\n",
        "                loss.backward()\n",
        "                if (itr + 1 ) % self.accumulation_steps == 0:\n",
        "                    self.optimizer.step()\n",
        "                    self.optimizer.zero_grad()\n",
        "            running_loss += loss.item()\n",
        "            outputs = outputs.detach().cpu()\n",
        "        epoch_loss = (running_loss * self.accumulation_steps) / total_batches\n",
        "        torch.cuda.empty_cache()\n",
        "        print(f'loss:{epoch_loss}')\n",
        "        return epoch_loss\n",
        "\n",
        "    def train_end(self):\n",
        "        train_loss = self.losses[\"train\"]\n",
        "        val_loss = self.losses[\"val\"]\n",
        "        df_data=np.array([train_loss,val_loss]).T\n",
        "        df = pd.DataFrame(df_data,columns = ['train_loss','val_loss'])\n",
        "        df.to_csv(\"Training_log.csv\")\n",
        "\n",
        "    def start(self):\n",
        "        for epoch in range(self.num_epochs):\n",
        "            self.iterate(epoch, \"train\")\n",
        "            state = {\n",
        "                \"epoch\": epoch,\n",
        "                \"best_loss\": self.best_loss,\n",
        "                \"state_dict\": self.net.state_dict(),\n",
        "                \"optimizer\": self.optimizer.state_dict(),\n",
        "            }\n",
        "            with torch.no_grad():\n",
        "                val_loss = self.iterate(epoch, \"val\")\n",
        "                self.scheduler.step(val_loss)\n",
        "            if val_loss < self.best_loss:\n",
        "                print(\"******** New optimal found, saving state ********\")\n",
        "                state[\"best_loss\"] = self.best_loss = val_loss\n",
        "                torch.save(state, \"./model.pth\")\n",
        "            print()\n",
        "            self.train_end()    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-6-9Ic87wRh",
        "colab_type": "text"
      },
      "source": [
        "#Getting the model and Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAvySwnnbBtL",
        "colab_type": "code",
        "outputId": "f211dfbd-a2f6-41c4-8484-7d9e96098dd5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "!pip install efficientnet_pytorch"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: efficientnet_pytorch in /usr/local/lib/python3.6/dist-packages (0.5.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from efficientnet_pytorch) (1.3.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch->efficientnet_pytorch) (1.17.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QR3KC3QxnvrB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from efficientnet_pytorch import EfficientNet"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLz1_p1AnvuF",
        "colab_type": "code",
        "outputId": "adb89c07-8d8f-44e3-92d7-f041bfa7bfea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model = EfficientNet.from_pretrained('efficientnet-b3')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded pretrained weights for efficientnet-b3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_nkhKX9tjs5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model = models.resnet50(pretrained = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzHzcXXAJBiP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_ftrs = model._fc.in_features\n",
        "model._fc = nn.Linear(num_ftrs, 3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WeHEI7Qziatm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_trainer = Trainer(model, CroppedImagesPath, df_train, df_val)\n",
        "model_trainer.start()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKOpfAxxnvxL",
        "colab_type": "code",
        "outputId": "492e00f9-09e4-45b1-e020-26979a5b5b64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.eval()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "EfficientNet(\n",
              "  (_conv_stem): Conv2dStaticSamePadding(\n",
              "    3, 40, kernel_size=(3, 3), stride=(2, 2), bias=False\n",
              "    (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
              "  )\n",
              "  (_bn0): BatchNorm2d(40, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "  (_blocks): ModuleList(\n",
              "    (0): MBConvBlock(\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        40, 40, kernel_size=(3, 3), stride=[1, 1], groups=40, bias=False\n",
              "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(40, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        40, 10, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        10, 40, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        40, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (1): MBConvBlock(\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        24, 24, kernel_size=(3, 3), stride=(1, 1), groups=24, bias=False\n",
              "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        24, 6, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        6, 24, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (2): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        144, 144, kernel_size=(3, 3), stride=[2, 2], groups=144, bias=False\n",
              "        (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (3): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        192, 192, kernel_size=(3, 3), stride=(1, 1), groups=192, bias=False\n",
              "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        192, 8, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        8, 192, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (4): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        192, 192, kernel_size=(3, 3), stride=(1, 1), groups=192, bias=False\n",
              "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        192, 8, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        8, 192, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (5): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        192, 192, kernel_size=(5, 5), stride=[2, 2], groups=192, bias=False\n",
              "        (static_padding): ZeroPad2d(padding=(1, 2, 1, 2), value=0.0)\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        192, 8, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        8, 192, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(48, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (6): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(288, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        288, 288, kernel_size=(5, 5), stride=(1, 1), groups=288, bias=False\n",
              "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(288, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        288, 12, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        12, 288, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(48, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (7): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(288, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        288, 288, kernel_size=(5, 5), stride=(1, 1), groups=288, bias=False\n",
              "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(288, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        288, 12, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        12, 288, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(48, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (8): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(288, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        288, 288, kernel_size=(3, 3), stride=[2, 2], groups=288, bias=False\n",
              "        (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(288, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        288, 12, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        12, 288, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(96, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (9): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(576, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        576, 576, kernel_size=(3, 3), stride=(1, 1), groups=576, bias=False\n",
              "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(576, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        576, 24, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        24, 576, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(96, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (10): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(576, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        576, 576, kernel_size=(3, 3), stride=(1, 1), groups=576, bias=False\n",
              "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(576, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        576, 24, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        24, 576, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(96, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (11): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(576, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        576, 576, kernel_size=(3, 3), stride=(1, 1), groups=576, bias=False\n",
              "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(576, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        576, 24, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        24, 576, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(96, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (12): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(576, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        576, 576, kernel_size=(3, 3), stride=(1, 1), groups=576, bias=False\n",
              "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(576, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        576, 24, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        24, 576, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(96, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (13): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(576, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        576, 576, kernel_size=(5, 5), stride=[1, 1], groups=576, bias=False\n",
              "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(576, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        576, 24, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        24, 576, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        576, 136, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(136, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (14): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(816, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        816, 816, kernel_size=(5, 5), stride=(1, 1), groups=816, bias=False\n",
              "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(816, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        816, 34, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        34, 816, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(136, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (15): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(816, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        816, 816, kernel_size=(5, 5), stride=(1, 1), groups=816, bias=False\n",
              "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(816, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        816, 34, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        34, 816, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(136, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (16): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(816, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        816, 816, kernel_size=(5, 5), stride=(1, 1), groups=816, bias=False\n",
              "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(816, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        816, 34, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        34, 816, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(136, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (17): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(816, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        816, 816, kernel_size=(5, 5), stride=(1, 1), groups=816, bias=False\n",
              "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(816, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        816, 34, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        34, 816, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        816, 136, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(136, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (18): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        136, 816, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(816, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        816, 816, kernel_size=(5, 5), stride=[2, 2], groups=816, bias=False\n",
              "        (static_padding): ZeroPad2d(padding=(1, 2, 1, 2), value=0.0)\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(816, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        816, 34, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        34, 816, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        816, 232, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(232, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (19): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        232, 1392, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(1392, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        1392, 1392, kernel_size=(5, 5), stride=(1, 1), groups=1392, bias=False\n",
              "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(1392, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        1392, 58, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        58, 1392, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        1392, 232, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(232, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (20): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        232, 1392, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(1392, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        1392, 1392, kernel_size=(5, 5), stride=(1, 1), groups=1392, bias=False\n",
              "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(1392, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        1392, 58, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        58, 1392, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        1392, 232, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(232, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (21): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        232, 1392, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(1392, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        1392, 1392, kernel_size=(5, 5), stride=(1, 1), groups=1392, bias=False\n",
              "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(1392, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        1392, 58, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        58, 1392, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        1392, 232, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(232, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (22): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        232, 1392, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(1392, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        1392, 1392, kernel_size=(5, 5), stride=(1, 1), groups=1392, bias=False\n",
              "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(1392, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        1392, 58, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        58, 1392, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        1392, 232, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(232, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (23): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        232, 1392, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(1392, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        1392, 1392, kernel_size=(5, 5), stride=(1, 1), groups=1392, bias=False\n",
              "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(1392, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        1392, 58, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        58, 1392, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        1392, 232, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(232, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (24): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        232, 1392, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(1392, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        1392, 1392, kernel_size=(3, 3), stride=[1, 1], groups=1392, bias=False\n",
              "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(1392, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        1392, 58, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        58, 1392, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        1392, 384, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(384, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "    (25): MBConvBlock(\n",
              "      (_expand_conv): Conv2dStaticSamePadding(\n",
              "        384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn0): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
              "        2304, 2304, kernel_size=(3, 3), stride=(1, 1), groups=2304, bias=False\n",
              "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
              "      )\n",
              "      (_bn1): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_se_reduce): Conv2dStaticSamePadding(\n",
              "        2304, 96, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_se_expand): Conv2dStaticSamePadding(\n",
              "        96, 2304, kernel_size=(1, 1), stride=(1, 1)\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_project_conv): Conv2dStaticSamePadding(\n",
              "        2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "        (static_padding): Identity()\n",
              "      )\n",
              "      (_bn2): BatchNorm2d(384, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "      (_swish): MemoryEfficientSwish()\n",
              "    )\n",
              "  )\n",
              "  (_conv_head): Conv2dStaticSamePadding(\n",
              "    384, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
              "    (static_padding): Identity()\n",
              "  )\n",
              "  (_bn1): BatchNorm2d(1536, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
              "  (_avg_pooling): AdaptiveAvgPool2d(output_size=1)\n",
              "  (_dropout): Dropout(p=0.3, inplace=False)\n",
              "  (_fc): Linear(in_features=1536, out_features=1000, bias=True)\n",
              "  (_swish): MemoryEfficientSwish()\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VylGPyuNBuHA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}