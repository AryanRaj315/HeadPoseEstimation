{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AUxneR2HX9R8"
   },
   "source": [
    "# Getting the Data and Unzipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9l7D4OEoZbVK"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "from math import atan2, asin\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import DataLoader, Dataset, sampler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import albumentations as aug\n",
    "from albumentations import (HorizontalFlip,VerticalFlip, ShiftScaleRotate, Normalize, Resize, Compose, GaussNoise,RandomRotate90,Transpose,RandomBrightnessContrast, RandomCrop)\n",
    "from albumentations.pytorch import ToTensor\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.models as models\n",
    "import time\n",
    "import scipy.io\n",
    "import random\n",
    "\n",
    "seed = 23\n",
    "random.seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "np.random.seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.set_default_tensor_type(\"torch.FloatTensor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fdEyYAZnYFeu"
   },
   "source": [
    "Getting the Path of train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "80uibl72aNuN"
   },
   "outputs": [],
   "source": [
    "def TrainPathList(path):\n",
    "  Img_Path = []\n",
    "  for root, dirs, files in os.walk(path, topdown=False):\n",
    "      for filename in files:\n",
    "        if(filename[-3:] == 'jpg'):\n",
    "          Img_Path.append(root+'/'+filename)\n",
    "  return Img_Path        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F2aIaHJHe30X"
   },
   "outputs": [],
   "source": [
    "def TestPathList(path):\n",
    "  Img_Path = []\n",
    "  for root, dirs, files in os.walk(path, topdown=False):\n",
    "      for filename in files:\n",
    "        if(filename[-3:] == 'jpg'):\n",
    "          Img_Path.append(root+filename)\n",
    "  return Img_Path        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainImagesPath = TrainPathList('300W_LP/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TestImagesPath = TestPathList('AFLW2000/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UeUGUpYGYQIz"
   },
   "source": [
    "Function to get Yaw, Pitch and Roll using Rotation matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/AryanRaj315/EfficientNet-PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IfqUoPPdaSsb"
   },
   "outputs": [],
   "source": [
    "def getAngles(path):\n",
    "    mat = scipy.io.loadmat(path)\n",
    "    angle_mat = mat['Pose_Para'][0]\n",
    "    pitch = angle_mat[0]*180/math.pi\n",
    "    yaw = angle_mat[1]*180/math.pi\n",
    "    roll = angle_mat[2]*180/math.pi\n",
    "    return np.array([yaw, pitch, roll]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l_Z7qICRYXZ5"
   },
   "source": [
    "# Class for Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EwQqRLcDaSu-"
   },
   "outputs": [],
   "source": [
    "class WLP300Dataset(Dataset):\n",
    "    def __init__(self, image_path):\n",
    "        self.phase = 'train'\n",
    "        self.path = image_path\n",
    "        self.transforms = get_transforms(self.phase)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        Image = cv2.imread(self.path[idx])\n",
    "        Angle = getAngles(self.path[idx][:-3]+'mat')\n",
    "        augmented = self.transforms(image=Image)\n",
    "        Image = augmented['image']\n",
    "        return Image, Angle\n",
    "\n",
    "    def __len__(self):\n",
    "          return len(self.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DFCZMJdYaSxz"
   },
   "outputs": [],
   "source": [
    "class AFLWDataset(Dataset):\n",
    "    def __init__(self, image_path, transforms= None):\n",
    "        self.phase = 'test'\n",
    "#         self.transform = transform\n",
    "        self.path = image_path\n",
    "        self.transforms = get_transforms(self.phase)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        Image = cv2.imread(self.path[idx]).astype('float32')\n",
    "        Angle = getAngles(self.path[idx][:-3]+'mat')\n",
    "        augmented = self.transforms(image=Image)\n",
    "        Image = augmented['image'] \n",
    "        return Image, Angle\n",
    "\n",
    "    def __len__(self):\n",
    "          return len(self.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms(phase):\n",
    "    list_transforms = []\n",
    "    if phase == \"train\":\n",
    "        list_transforms.extend(\n",
    "        [\n",
    "         aug.OneOf([\n",
    "             aug.RandomContrast(),\n",
    "             aug.RandomGamma(),\n",
    "             aug.RandomBrightness(),\n",
    "             ], p=1),\n",
    "         Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225], p=1)\n",
    "        ]\n",
    "    )\n",
    "        #In case of test\n",
    "    list_transforms.extend(\n",
    "        [\n",
    "            Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225], p=1),\n",
    "            ToTensor(),\n",
    "        ]\n",
    "    )\n",
    "    list_trfms = Compose(list_transforms)\n",
    "    return list_trfms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K7zYJj8faS9K"
   },
   "outputs": [],
   "source": [
    "def provider(phase, image_list, batch_size=8, num_workers=0):  \n",
    "    if phase == 'test':\n",
    "        image_dataset = AFLWDataset(image_list)\n",
    "    else:    \n",
    "        image_dataset = WLP300Dataset(image_list)\n",
    "    dataloader = DataLoader(\n",
    "        image_dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        shuffle=True,   \n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qz3deOd9YgeO"
   },
   "source": [
    "# Class for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4o7ecbkuaS_c"
   },
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    '''This class takes care of training and validpathation of our model'''\n",
    "    def __init__(self, model, Train_image_list,Test_image_list, bs, lr, epochs):\n",
    "        self.batch_size = bs\n",
    "        self.accumulation_steps = 1\n",
    "        self.lr = lr\n",
    "        self.num_epochs = epochs\n",
    "        self.best_loss = float(\"inf\")\n",
    "        self.phases = [\"train\", \"val\"]\n",
    "        self.device = torch.device(\"cuda:0\")\n",
    "        torch.set_default_tensor_type(\"torch.FloatTensor\")\n",
    "        self.net = model\n",
    "        self.image_list = {'train': Train_image_list, 'val': Test_image_list}\n",
    "        self.losses = {phase: [] for phase in self.phases}\n",
    "        self.criterion = torch.nn.L1Loss()\n",
    "        self.optimizer = optim.Adam(self.net.parameters(), lr=self.lr)\n",
    "        self.scheduler = ReduceLROnPlateau(self.optimizer, mode=\"min\", patience=3, verbose=True,factor = 0.5,min_lr = 1e-5)\n",
    "#         self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max, eta_min=5e-5, last_epoch=-1)\n",
    "        self.net = self.net.to(self.device)\n",
    "        cudnn.benchmark = True\n",
    "        self.dataloaders = {\n",
    "            phase: provider(\n",
    "                phase=phase,\n",
    "                image_list = self.image_list[phase],\n",
    "                batch_size=self.batch_size\n",
    "            )\n",
    "            for phase in self.phases\n",
    "        }\n",
    "        self.losses = {phase: [] for phase in self.phases}\n",
    "        \n",
    "    def forward(self, images, targets):\n",
    "        images = images.to(self.device)\n",
    "        targets = targets.type(\"torch.FloatTensor\")\n",
    "        targets = targets.to(self.device)\n",
    "        yaw, pitch, roll = self.net(images)\n",
    "        preds = torch.cat((yaw, pitch, roll), 1)\n",
    "#         preds = torch.Tensor([yaw,pitch,roll])\n",
    "        preds.to(self.device)\n",
    "        loss = self.criterion(preds,targets)\n",
    "#         loss_yaw = self.criterion(yaw, target[:,0])\n",
    "#         loss_pitch = self.criterion(pitch, target[:,1])\n",
    "#         loss_roll = self.criterion(roll, target[:,2])\n",
    "#         loss_yaw = torch.mean(torch.stack([loss_pitch, loss_roll]))\n",
    "        return loss\n",
    "\n",
    "    def iterate(self, epoch, phase):\n",
    "        start = time.strftime(\"%H:%M:%S\")\n",
    "        print(f\"Starting epoch: {epoch} | phase: {phase} | ⏰: {start}\")\n",
    "        batch_size = self.batch_size\n",
    "        self.net.train(phase == \"train\")\n",
    "        dataloader = self.dataloaders[phase]\n",
    "        running_loss = 0.0\n",
    "        total_batches = len(dataloader)\n",
    "        tk0 = tqdm(dataloader, total=total_batches)\n",
    "        self.optimizer.zero_grad()\n",
    "        for itr, batch in enumerate(tk0): # replace `dataloader` with `tk0` for tqdm\n",
    "            images, targets = batch\n",
    "            loss= self.forward(images, targets)\n",
    "            loss = loss / self.accumulation_steps\n",
    "            if phase == \"train\":\n",
    "                loss.backward()\n",
    "                if (itr + 1 ) % self.accumulation_steps == 0:\n",
    "                    self.optimizer.step()\n",
    "                    self.optimizer.zero_grad()\n",
    "            running_loss += loss.item()\n",
    "#             outputs = outputs.detach().cpu()\n",
    "            tk0.set_postfix(loss=(running_loss / ((itr + 1))))\n",
    "        epoch_loss = (running_loss * self.accumulation_steps) / total_batches\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f'loss:{epoch_loss}')\n",
    "        return epoch_loss\n",
    "\n",
    "    def train_end(self):\n",
    "        train_loss = self.losses[\"train\"]\n",
    "        val_loss = self.losses[\"val\"]\n",
    "        df_data=np.array([train_loss,val_loss]).T\n",
    "        df = pd.DataFrame(df_data,columns = ['train_loss','val_loss'])\n",
    "        df.to_csv(\"model-expand-att.csv\")\n",
    "\n",
    "    def start(self):\n",
    "        for epoch in range(self.num_epochs):\n",
    "            train_loss = self.iterate(epoch, \"train\")\n",
    "            self.losses[\"train\"].append(train_loss)\n",
    "            state = {\n",
    "                \"epoch\": epoch,\n",
    "                \"best_loss\": self.best_loss,\n",
    "                \"state_dict\": self.net.state_dict(),\n",
    "                \"optimizer\": self.optimizer.state_dict(),\n",
    "            }\n",
    "            with torch.no_grad():\n",
    "                val_loss = self.iterate(epoch, \"val\")\n",
    "                self.losses[\"val\"].append(val_loss)\n",
    "                self.scheduler.step(val_loss)\n",
    "            if val_loss < self.best_loss:\n",
    "                print(\"******** New optimal found, saving state ********\")\n",
    "                state[\"best_loss\"] = self.best_loss = val_loss\n",
    "                torch.save(state, \"./model-expand-att.pth\")\n",
    "            print()\n",
    "            self.train_end()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "ZdX3q9sfaTCj",
    "outputId": "c8025d63-16d8-4241-9aff-4332de756f28"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, 'vision/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'vision'...\n",
      "remote: Enumerating objects: 22, done.\u001b[K\n",
      "remote: Counting objects: 100% (22/22), done.\u001b[K\n",
      "remote: Compressing objects: 100% (19/19), done.\u001b[K\n",
      "remote: Total 6193 (delta 7), reused 7 (delta 3), pack-reused 6171\u001b[K\n",
      "Receiving objects: 100% (6193/6193), 9.62 MiB | 17.07 MiB/s, done.\n",
      "Resolving deltas: 100% (4128/4128), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone 'https://github.com/AryanRaj315/vision.git'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ORxv1XeXaTFg"
   },
   "outputs": [],
   "source": [
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "o4YKLA_vaTI-",
    "outputId": "600ddaa8-d910-4b27-a667-829dbded56cf"
   },
   "outputs": [],
   "source": [
    "model = models.resnet50()#.from_pretrained('efficientnet-b0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def matplotlib_imshow(img, one_channel=False):\n",
    "#     if one_channel:\n",
    "#         img = img.mean(dim=0)\n",
    "#     img = img / 2 + 0.5     # unnormalize\n",
    "#     npimg = img.numpy()\n",
    "#     if one_channel:\n",
    "#         plt.imshow(npimg)\n",
    "#     else:\n",
    "#         plt.imshow(np.transpose(npimg, (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "# writer = SummaryWriter('runs/300W_LP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainloader = provider(\"train\", TrainImagesPath)\n",
    "# # get some random training images\n",
    "# dataiter = iter(trainloader)\n",
    "# images, labels = dataiter.next()\n",
    "\n",
    "# # create grid of images\n",
    "# img_grid = torchvision.utils.make_grid(images)\n",
    "\n",
    "# # show images\n",
    "# matplotlib_imshow(img_grid, one_channel=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writer.add_image('300W_LP/', img_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writer.add_graph(model, images)\n",
    "# writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tensorboard --logdir=runs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget 'http://storage.googleapis.com/public-models/efficientnet/efficientnet-b4-6ed6700e.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_dict = torch.load('efficientnet-b4-6ed6700e.pth')\n",
    "IncompatibleKeys = model.load_state_dict(pretrained_dict,strict = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "UnloadedWeights = IncompatibleKeys[0][:-21]\n",
    "AvailableWeights = IncompatibleKeys[1][8:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i in range(len(UnloadedWeights)):\n",
    "    if AvailableWeights[count][-19:] == 'num_batches_tracked':\n",
    "        count+=1\n",
    "        model.state_dict().get(UnloadedWeights[i], None).data = pretrained_dict[AvailableWeights[count]]\n",
    "    count += 1\n",
    "    if(count == 109):\n",
    "        count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wk5Mro1IaTL_"
   },
   "outputs": [],
   "source": [
    "num_ftrs = model._fc_p1.in_features\n",
    "model._fc_p1 = nn.Linear(num_ftrs, 1)\n",
    "model._fc_p2 = nn.Linear(num_ftrs, 1)\n",
    "model._fc_p3 = nn.Linear(num_ftrs, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "5a54c91af3284933b49500e63b1ed262",
      "8efc7db7a0794e1e8d6e6cc6ed6e1f23",
      "ba0b2918c2304cd784f39b59617217fd",
      "29f764bb5285483d897fed9afd3f2d9b",
      "9cdaec13f8a84b658ec5ac5a6a5852cd",
      "ea3e1e004bc747fc893f641306d532cf",
      "ef45c53e92eb4d7daac3280fde01d09d",
      "3a1f958fbaa04dc494b0fa98518d0d3d"
     ]
    },
    "colab_type": "code",
    "id": "EsIuWurbaTPG",
    "outputId": "29832333-e230-46dc-9f7b-401dba431670"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch: 0 | phase: train | ⏰: 10:00:59\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18027eeb92164c9ca100c23fda968631",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=15307.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_trainer = Trainer(model, TrainImagesPath, TestImagesPath, 8,1e-3,5)\n",
    "model_trainer.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torch.load()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Headpose_300wlp(Train)_AFLW(Test).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "29f764bb5285483d897fed9afd3f2d9b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3a1f958fbaa04dc494b0fa98518d0d3d",
      "placeholder": "​",
      "style": "IPY_MODEL_ef45c53e92eb4d7daac3280fde01d09d",
      "value": " 27% 2095/7654 [19:59&lt;52:29,  1.77it/s, loss=3.1]"
     }
    },
    "3a1f958fbaa04dc494b0fa98518d0d3d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5a54c91af3284933b49500e63b1ed262": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ba0b2918c2304cd784f39b59617217fd",
       "IPY_MODEL_29f764bb5285483d897fed9afd3f2d9b"
      ],
      "layout": "IPY_MODEL_8efc7db7a0794e1e8d6e6cc6ed6e1f23"
     }
    },
    "8efc7db7a0794e1e8d6e6cc6ed6e1f23": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9cdaec13f8a84b658ec5ac5a6a5852cd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ba0b2918c2304cd784f39b59617217fd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ea3e1e004bc747fc893f641306d532cf",
      "max": 7654,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9cdaec13f8a84b658ec5ac5a6a5852cd",
      "value": 2095
     }
    },
    "ea3e1e004bc747fc893f641306d532cf": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ef45c53e92eb4d7daac3280fde01d09d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
